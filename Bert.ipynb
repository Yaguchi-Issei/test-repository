{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#GPU使用可能確認\n",
    "print(torch.cuda.is_available())\n",
    "#使用できるデバイス数確認\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 30 13:31:51 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:81:00.0  On |                  N/A |\n",
      "| 54%   51C    P8    42W / 350W |   9784MiB / 24576MiB |      7%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:C1:00.0 Off |                  N/A |\n",
      "| 42%   49C    P8    19W / 350W |   5700MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2157250      G   /usr/lib/xorg/Xorg                 53MiB |\n",
      "|    0   N/A  N/A   2815414      G   /usr/lib/xorg/Xorg                150MiB |\n",
      "|    0   N/A  N/A   2957660      C   ...envs/[yaguchi]/bin/python     7477MiB |\n",
      "|    0   N/A  N/A   2957679      G   /usr/bin/gnome-shell               84MiB |\n",
      "|    0   N/A  N/A   2963016      C   ...envs/[yaguchi]/bin/python     1999MiB |\n",
      "|    1   N/A  N/A   2157250      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   2815414      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   2957660      C   ...envs/[yaguchi]/bin/python     5687MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: mojimoji in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (0.0.12)\n",
      "Requirement already satisfied: pyknp in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (0.6.1)\n",
      "Requirement already satisfied: six in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from pyknp) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install mojimoji\n",
    "! pip install pyknp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import mojimoji\n",
    "import pandas as pd\n",
    "from transformers import BertConfig, BertModel, BertTokenizer\n",
    "from pyknp import Juman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###　ターミナルで形態素解析juman++のインストール\n",
    "https://qiita.com/Gushi_maru/items/ee434b5bc9f020c8feb6\n",
    "・jumanpp-2.0.0-rc3 download\n",
    "wget https://github.com/ku-nlp/jumanpp/releases/download/v2.0.0-rc3/jumanpp-2.0.0-rc3.tar.xz\n",
    "・unzip a file\n",
    "tar xvf jumanpp-2.0.0-rc3.tar.xz\n",
    "・コンパイラのインストール\n",
    "sudo apt update -y\n",
    "sudo apt upgrade -y\n",
    "sudo apt install build-essential -y\n",
    "・cmakeインストール\n",
    "sudo apt install cmake -y\n",
    "・コンパイラ用フォルダ作成\n",
    "cd [jumanディレクトリのパスの指定]\n",
    "mkdir build\n",
    "・juman++コンパイル\n",
    "cd [buildのパス指定]\n",
    "cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local\n",
    "make\n",
    "・コンパイル完了したらインストール開始\n",
    "sudo make install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "すもも すもも すもも 名詞 6 普通名詞 1 * 0 * 0 \"自動獲得:テキスト\"\n",
      "@ すもも すもも すもも 名詞 6 普通名詞 1 * 0 * 0 \"代表表記:酸桃/すもも 自動獲得:EN_Wiktionary\"\n",
      "も も も 助詞 9 副助詞 2 * 0 * 0 NIL\n",
      "もも もも もも 名詞 6 普通名詞 1 * 0 * 0 \"代表表記:桃/もも ドメイン:料理・食事 カテゴリ:植物;人工物-食べ物 漢字読み:訓\"\n",
      "@ もも もも もも 名詞 6 普通名詞 1 * 0 * 0 \"代表表記:股/もも カテゴリ:動物-部位\"\n",
      "も も も 助詞 9 副助詞 2 * 0 * 0 NIL\n",
      "もも もも もも 名詞 6 普通名詞 1 * 0 * 0 \"代表表記:桃/もも ドメイン:料理・食事 カテゴリ:植物;人工物-食べ物 漢字読み:訓\"\n",
      "@ もも もも もも 名詞 6 普通名詞 1 * 0 * 0 \"代表表記:股/もも カテゴリ:動物-部位\"\n",
      "も も も 助詞 9 副助詞 2 * 0 * 0 NIL\n",
      "もも もも もも 名詞 6 普通名詞 1 * 0 * 0 \"代表表記:桃/もも ドメイン:料理・食事 カテゴリ:植物;人工物-食べ物 漢字読み:訓\"\n",
      "@ もも もも もも 名詞 6 普通名詞 1 * 0 * 0 \"代表表記:股/もも カテゴリ:動物-部位\"\n",
      "EOS\n"
     ]
    }
   ],
   "source": [
    "#juman++動作確認\n",
    "! echo すももももももももももも | jumanpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jiuman++のバージョン確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juman++ Version: 2.0.0-rc3 / Dictionary: 20190731-356e143 / LM: K:20190430-7d143fb L:20181122-b409be68 F:20171214-9d125cb\n"
     ]
    }
   ],
   "source": [
    "! jumanpp -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然言語処理のBERTについて学習中です。\n",
      "['自然', '言語', '処理', 'の', 'BERT', 'に', 'ついて', '学習', '中', 'です', '。']\n"
     ]
    }
   ],
   "source": [
    "# JUMANの動作確認\n",
    "from pyknp import Juman\n",
    "text = \"自然言語処理のBERTについて学習中です。\"\n",
    "juman = Juman()\n",
    "result =juman.analysis(text)\n",
    "result = [mrph.midasi for mrph in result.mrph_list()]\n",
    "print(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ダウンロードしたモデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "vocab_file = \"/home/sakulab/workspace/yaguchi/research/Master/code/LLM/Japanese_L-12_H-768_A-12_E-30_BPE 2/vocab.txt\"\n",
    "config_file = \"/home/sakulab/workspace/yaguchi/research/Master/code/LLM/Japanese_L-12_H-768_A-12_E-30_BPE 2/bert_config.json\"\n",
    "model_file = \"/home/sakulab/workspace/yaguchi/research/Master/code/LLM/Japanese_L-12_H-768_A-12_E-30_BPE 2/pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer(vocab_file, do_lower_case=False, do_basic_tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyknp import Juman\n",
    "\n",
    "class JumanTokenizer():\n",
    "    def __init__(self):\n",
    "        self.juman = Juman()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        result = self.juman.analysis(text)\n",
    "        return [mrph.midasi for mrph in result.mrph_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "juman_tokenizer = JumanTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "    \n",
    "def preprocessing_text(text):\n",
    "       \n",
    "        text = mojimoji.han_to_zen(text) \n",
    "        \n",
    "        text = re.sub(r'[0-9 ０-９]+', '0', text)  \n",
    "\n",
    "        text = re.sub('\\r', '', text)\n",
    "        text = re.sub('\\n', '', text)\n",
    "\n",
    "        text = text.replace(\"、\", \" \")\n",
    "        text = text.replace(\"。\", \" \")\n",
    "        \n",
    "        for p in string.punctuation:\n",
    "            if (p == \".\") or (p == \",\"):\n",
    "                continue\n",
    "            else:\n",
    "                text = text.replace(p, \" \")\n",
    "            return text\n",
    "\n",
    "def tokenizer_with_preprocessing(text):\n",
    "        text = preprocessing_text(text)\n",
    "        tokens = juman_tokenizer.tokenize(text)\n",
    "        bert_tokens = bert_tokenizer.tokenize(\" \".join(tokens))\n",
    "        ids = bert_tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens+ [\"[SEP]\"])\n",
    "        return ids\n",
    "\n",
    "def preprocessing_sentences(text):\n",
    "        for i in range(len(text)):\n",
    "            text[i] = preprocessing_text(text[i])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作成したデータの読み込み\n",
    "df_train = pd.read_csv(\"/home/sakulab/workspace/yaguchi/research/Master/data/意見データ(5分割交差検証用)/train_data(5).csv\", header=0)\n",
    "#df_train = df_train.dropna(how='any') # nanのところは落とす（欠損値の削除）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データサイズ： (7000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>ユニバーサルスタジオハリウッド の スタジオツアーノーカット 動画 って の 観 て たら ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>もう 5年 も 一緒 に いる ん だ よ 、 、 次 は Disney か な ??? #...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>ハウステンボス 光 の 王国 大 迫力 の 綺麗 すぎる くらい の イルミ だっ た ずっ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>ユニバ３回ぐらいしか行ったことない私ですら迷わんかったぞ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5845</th>\n",
       "      <td>ディズニー、クリスマスにも行こうよって。どこまで本気なのかわからないけど実現したらいいな。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>休日のパワーチャージは、コンラッド東京の限定ブランチで。\\n＜ＵＲＬ＞</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2870</th>\n",
       "      <td>俺だって恋人とディズニー行ってカチューシャ付けたりしたい！！！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6838</th>\n",
       "      <td>ＴＤＲ？？？東京ディズニーランド？？？\\n（´・ε・｀）？？？\\nがっつりＵＳＪ派だわ\\n\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4806</th>\n",
       "      <td>超特急 ユニバーサル・スタジオ・ジャパン ライブ チケット 譲 : 12 / 25 スタンデ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>a ー nation 2018 【 求 】 8 / 4 長崎 ハウステンボス 2 連 A 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "5210  ユニバーサルスタジオハリウッド の スタジオツアーノーカット 動画 って の 観 て たら ...      1\n",
       "1781  もう 5年 も 一緒 に いる ん だ よ 、 、 次 は Disney か な ??? #...      0\n",
       "674   ハウステンボス 光 の 王国 大 迫力 の 綺麗 すぎる くらい の イルミ だっ た ずっ...      1\n",
       "157                        ユニバ３回ぐらいしか行ったことない私ですら迷わんかったぞ      0\n",
       "5845      ディズニー、クリスマスにも行こうよって。どこまで本気なのかわからないけど実現したらいいな。      0\n",
       "2265                休日のパワーチャージは、コンラッド東京の限定ブランチで。\\n＜ＵＲＬ＞      0\n",
       "2870                    俺だって恋人とディズニー行ってカチューシャ付けたりしたい！！！      0\n",
       "6838  ＴＤＲ？？？東京ディズニーランド？？？\\n（´・ε・｀）？？？\\nがっつりＵＳＪ派だわ\\n\\...      0\n",
       "4806  超特急 ユニバーサル・スタジオ・ジャパン ライブ チケット 譲 : 12 / 25 スタンデ...      0\n",
       "1574  a ー nation 2018 【 求 】 8 / 4 長崎 ハウステンボス 2 連 A 2...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# データの確認\n",
    "print(f'データサイズ： {df_train.shape}')\n",
    "display(df_train.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作成したデータの読み込み\n",
    "df_test = pd.read_csv(\"/home/sakulab/workspace/yaguchi/research/Master/data/意見データ(5分割交差検証用)/test_data(5).csv\",header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データサイズ： (1998, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>昨日 の 夜 mommy に 日曜 ディズニー の パレード ??? って 目 輝か せ て...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>この 前 根岸 さん に 、 ねぇ ! ディズニー 行か ない の !? ディズニー !! ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>ユニバいきたい\\n\\n早く\\n年パスを買わなきゃ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>来月 は ディズニー ヾ (@⌒ ー ⌒@) ノ ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>ユニバ の ハリポタワールド の グッズ 調べ た けど なかなか なお 値段 する みたい...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>プリクラとってる時にディズニーのチケットのサプライズ死ぬまでにされてみたい</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>マツコの知らない世界 の ディズニーシー 特集 すごい おもしろかっ た 。 なぜ そう な...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>昨日友達とオルゴールミュージアム・ピクサー展行った！！\\nディズニーメドレーがちよかったわ\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>【レゴランド】来場者低迷、隣接店舗が２か月で閉店売り上げが想定の１０分の１</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>マカティシャングリラマニラ ( Makati Shangri ー La ) に 宿泊 し ま...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "1495  昨日 の 夜 mommy に 日曜 ディズニー の パレード ??? って 目 輝か せ て...      0\n",
       "650   この 前 根岸 さん に 、 ねぇ ! ディズニー 行か ない の !? ディズニー !! ...      1\n",
       "196                            ユニバいきたい\\n\\n早く\\n年パスを買わなきゃ      0\n",
       "1507                         来月 は ディズニー ヾ (@⌒ ー ⌒@) ノ ?      0\n",
       "1436  ユニバ の ハリポタワールド の グッズ 調べ た けど なかなか なお 値段 する みたい...      1\n",
       "184               プリクラとってる時にディズニーのチケットのサプライズ死ぬまでにされてみたい      0\n",
       "1010  マツコの知らない世界 の ディズニーシー 特集 すごい おもしろかっ た 。 なぜ そう な...      1\n",
       "31    昨日友達とオルゴールミュージアム・ピクサー展行った！！\\nディズニーメドレーがちよかったわ\\...      1\n",
       "242               【レゴランド】来場者低迷、隣接店舗が２か月で閉店売り上げが想定の１０分の１      0\n",
       "919   マカティシャングリラマニラ ( Makati Shangri ー La ) に 宿泊 し ま...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'データサイズ： {df_test.shape}')\n",
    "display(df_test.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### valデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作成したデータの読み込み\n",
    "df_val = pd.read_csv(\"/home/sakulab/workspace/yaguchi/research/Master/data/意見データ(5分割交差検証用)/val_data(5).csv\",header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データサイズ： (996, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>【 生放送 】 USJ 及び 大阪 を 勉強 する 枠 を 開始 し まし た 。 &lt; UR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>ディズニーランド クリスマス 関連 ブログ 更新 中 ! よろしく です &lt; URL &gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>2018-10-06t21:21:21z新潟地方気象台【天気概況】佐渡では、7日朝まで暴風に...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>次 ユニバ 行く 時 、 レイブンクロー か スリザリン の ネクタイ 買う ねん はあ 、...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>バンドアレンジしてみましたー（･∀･）！！よかったら聴いてみてね！！【全１０曲メドレー】ディ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>6月 の 大阪 楽しみ すぎ て … コンラッド 予約 取れ た ＼ (^o^)/ 5月 は...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Ｄｉｓｎｅｙ行った時に撮った風景のやつ\\n\\nつぶやき今日始めたばっかでフォロワー少ないから...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>え 、 この ディズニー の バッグ 見 た こと ない と 思っ て 確認 し たら 香港...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>6 . 3 な あや と ユニバ 行っ て き た ! 暑 すぎ ほんま に 、 絶対 焼け...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>2日目 は モーニング バイキング し て 日光江戸村 へ 。 せっかく だし と 女 剣士...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "432  【 生放送 】 USJ 及び 大阪 を 勉強 する 枠 を 開始 し まし た 。 < UR...      0\n",
       "546       ディズニーランド クリスマス 関連 ブログ 更新 中 ! よろしく です < URL >      0\n",
       "907  2018-10-06t21:21:21z新潟地方気象台【天気概況】佐渡では、7日朝まで暴風に...      0\n",
       "375  次 ユニバ 行く 時 、 レイブンクロー か スリザリン の ネクタイ 買う ねん はあ 、...      1\n",
       "50   バンドアレンジしてみましたー（･∀･）！！よかったら聴いてみてね！！【全１０曲メドレー】ディ...      1\n",
       "460  6月 の 大阪 楽しみ すぎ て … コンラッド 予約 取れ た ＼ (^o^)/ 5月 は...      1\n",
       "259  Ｄｉｓｎｅｙ行った時に撮った風景のやつ\\n\\nつぶやき今日始めたばっかでフォロワー少ないから...      1\n",
       "746  え 、 この ディズニー の バッグ 見 た こと ない と 思っ て 確認 し たら 香港...      0\n",
       "388  6 . 3 な あや と ユニバ 行っ て き た ! 暑 すぎ ほんま に 、 絶対 焼け...      1\n",
       "835  2日目 は モーニング バイキング し て 日光江戸村 へ 。 せっかく だし と 女 剣士...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'データサイズ： {df_val.shape}')\n",
    "display(df_val.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  え 、 ピューロランド 行き たい 。 。 。 誰か 連れ て っ て ← 絶対 SKY ー HI 可愛い やん 。 いや 、 カッコイイ パターン も あり ?\n",
      "Tokenized:  ['え', '\\u3000', '\\u3000', '\\u3000', 'ピュー', 'ロランド', '\\u3000', '行き', '\\u3000', 'たい', '\\u3000', '\\u3000', '\\u3000', '\\u3000', '\\u3000', '\\u3000', '\\u3000', '誰', 'か', '\\u3000', '連れ', '\\u3000', 'て', '\\u3000っ', '\\u3000', 'て', '\\u3000', '←', '\\u3000', '絶対', '\\u3000', 'ＳＫＹ', '\\u3000', 'ー', '\\u3000', 'ＨＩ', '\\u3000', '可愛い', '\\u3000', 'やん', '\\u3000', '\\u3000', '\\u3000', 'いや', '\\u3000', '\\u3000', '\\u3000', 'カッコイイ', '\\u3000', 'パターン', '\\u3000', 'も', '\\u3000', 'あり', '\\u3000', '？']\n",
      "Token IDs:  [2, 2639, 1, 1, 1, 16249, 1, 1, 2003, 1, 755, 1, 1, 1, 1, 1, 1, 1, 2437, 90, 1, 6550, 1, 3402, 1, 1, 3402, 1, 1, 1, 4073, 1, 1, 1, 20338, 1, 24204, 1, 29850, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4710, 1, 23, 1, 125, 1, 1566, 3]\n"
     ]
    }
   ],
   "source": [
    "# モデルに飲ませるデータと、ラベルを準備\n",
    "text_train = df_train.text.values\n",
    "labels_train = df_train.label.values\n",
    "\n",
    "\n",
    "## 確認\n",
    "print(' Original: ', text_train[0])\n",
    "print('Tokenized: ', juman_tokenizer.tokenize(preprocessing_text(text_train[0])))\n",
    "print('Token IDs: ', tokenizer_with_preprocessing(text_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  そういえば大阪のウンタラではＵＳＪのウンタラをウンタラするそうですね\n",
      "なるほどこの世界はなろう小説でしたか\n",
      "Tokenized:  ['そう', 'いえば', '大阪', 'の', 'ウンタラ', 'で', 'は', 'ＵＳＪ', 'の', 'ウンタラ', 'を', 'ウンタラ', 'する', 'そうです', 'ね', 'なるほど', 'この', '世界', 'は', 'なろう', '小説', 'でした', 'か']\n",
      "Token IDs:  [2, 2208, 10922, 340, 5, 1, 13, 9, 1, 5, 1, 10, 1, 22, 1, 2382, 1, 46, 118, 9, 17070, 728, 14947, 90, 3]\n"
     ]
    }
   ],
   "source": [
    "# モデルに飲ませるデータと、ラベルを準備\n",
    "text_val = df_val.text.values\n",
    "labels_val = df_val.label.values\n",
    "\n",
    "## 確認\n",
    "print(' Original: ', text_val[0])\n",
    "print('Tokenized: ', juman_tokenizer.tokenize(preprocessing_text(text_val[0])))\n",
    "print('Token IDs: ', tokenizer_with_preprocessing(text_val[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  レゴランドの向いにホテルが立ってるんですよー\n",
      "私も今日、初めて行きました；；\n",
      "レストランはビュッフェタイプでした。\n",
      "他にもバーとかありましたよ？\n",
      "少々お高いですが、わりと楽しめました！\n",
      "Tokenized:  ['レゴ', 'ランド', 'の', '向い', 'に', 'ホテル', 'が', '立って', 'る', 'んです', 'よー', '私', 'も', '今日', '\\u3000', '初めて', '行き', 'ました', '；', '；', 'レストラン', 'は', 'ビュッフェ', 'タイプ', 'でした', '\\u3000', '他', 'に', 'も', 'バー', 'と', 'か', 'あり', 'ました', 'よ', '？', '少々', 'お', '高い', 'です', 'が', '\\u3000', 'わりと', '楽しめ', 'ました', '！']\n",
      "Token IDs:  [2, 1, 5802, 5, 1, 8, 1762, 11, 7234, 3282, 10307, 1, 1038, 23, 2281, 1, 526, 2003, 4561, 4123, 4123, 4825, 9, 1, 1483, 14947, 1, 151, 8, 23, 1304, 12, 90, 125, 4561, 1291, 1566, 23151, 273, 537, 3338, 11, 1, 1, 1, 4561, 228, 3]\n"
     ]
    }
   ],
   "source": [
    "# モデルに飲ませるデータと、ラベルを準備\n",
    "text_test = df_test.text.values\n",
    "labels_test = df_test.label.values\n",
    "\n",
    "## 確認\n",
    "print(' Original: ', text_test[0])\n",
    "print('Tokenized: ', juman_tokenizer.tokenize(preprocessing_text(text_test[0])))\n",
    "print('Token IDs: ', tokenizer_with_preprocessing(text_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing:  え　 　ピューロランド　行き　たい　 　 　 　誰か　連れ　て　っ　て　←　絶対　ＳＫＹ　ー　ＨＩ　可愛い　やん　 　いや　 　カッコイイ　パターン　も　あり　？\n",
      "Preprocessing:  そういえば大阪のウンタラではＵＳＪのウンタラをウンタラするそうですねなるほどこの世界はなろう小説でしたか\n",
      "Preprocessing:  レゴランドの向いにホテルが立ってるんですよー私も今日 初めて行きました；；レストランはビュッフェタイプでした 他にもバーとかありましたよ？少々お高いですが わりと楽しめました！\n"
     ]
    }
   ],
   "source": [
    "text_train = preprocessing_sentences(text_train)\n",
    "text_val = preprocessing_sentences(text_val)\n",
    "text_test = preprocessing_sentences(text_test)\n",
    "print('Preprocessing: ', text_train[0])\n",
    "print('Preprocessing: ', text_val[0])\n",
    "print('Preprocessing: ', text_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDへ変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  え　 　ピューロランド　行き　たい　 　 　 　誰か　連れ　て　っ　て　←　絶対　ＳＫＹ　ー　ＨＩ　可愛い　やん　 　いや　 　カッコイイ　パターン　も　あり　？\n",
      "Token IDs: tensor([    2,  2639, 16249,   598,  2401,  2003,   755,  2437,   856,  6550,\n",
      "         3402, 14443,  3402,     1,  4073, 14229,  1192, 20338, 24204, 29850,\n",
      "           34,  2029,   142,  2143, 22565,   643,   643,  4710,    23,   125,\n",
      "         1566,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "input_ids_train = []\n",
    "attention_masks_train = []\n",
    "max_len = 128\n",
    "\n",
    "# 1文づつ処理\n",
    "for sentence in text_train:\n",
    "    encoded_dict_train = bert_tokenizer.encode_plus( # input_ids, token_type_ids, attention_mask を出力する\n",
    "                        sentence,                      \n",
    "                        add_special_tokens = True, # Special Tokenの追加\n",
    "                        max_length = max_len + 2,           # 文章の長さを固定（Padding/Trancatinating）\n",
    "                        pad_to_max_length = True,# PADDINGで埋める\n",
    "                        return_attention_mask = True,   # Attention maskの作成\n",
    "                        return_tensors = 'pt',     #  Pytorch tensorsで返す\n",
    "                   )\n",
    "\n",
    "    input_ids_train.append(encoded_dict_train['input_ids'])\n",
    "    attention_masks_train.append(encoded_dict_train['attention_mask']) # 実質的なトークンがある部分のみ１になる\n",
    "\n",
    "#input_ids_train.append(sentences_train['input_ids'])\n",
    "#attention_masks_train.append(sentences_train['attention_mask'])\n",
    "\n",
    "# リストに入ったtensorを縦方向（dim=0）へ結合\n",
    "input_ids_train = torch.cat(input_ids_train, dim=0)\n",
    "attention_masks_train = torch.cat(attention_masks_train, dim=0)\n",
    "\n",
    "# tensor型に変換\n",
    "labels_train = torch.tensor(labels_train)\n",
    "\n",
    "# 確認\n",
    "print('Original: ', text_train[0])\n",
    "print('Token IDs:', input_ids_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  そういえば大阪のウンタラではＵＳＪのウンタラをウンタラするそうですねなるほどこの世界はなろう小説でしたか\n",
      "Token IDs: tensor([    2,  2208,   574,  9830,  1448, 11136,   422,  7070, 22644,   429,\n",
      "         7019,  5901,  2007,   422,  7070, 22644, 25246,  7070, 22644,  3244,\n",
      "         3581, 12323,  3861, 10124, 17108,  1126,   422,  3390,  4971,  7019,\n",
      "          275,  3958,  5574,  5748,   429,  1033,   856,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "input_ids_val = []\n",
    "attention_masks_val = []\n",
    "max_len = 128\n",
    "\n",
    "\n",
    "# 1文づつ処理\n",
    "for sentence in text_val:\n",
    "    encoded_dict_val = bert_tokenizer.encode_plus( # input_ids, token_type_ids, attention_mask を出力する\n",
    "                        sentence,                      \n",
    "                        add_special_tokens = True, # Special Tokenの追加\n",
    "                        max_length = max_len + 2,           # 文章の長さを固定（Padding/Trancatinating）\n",
    "                        pad_to_max_length = True,# PADDINGで埋める\n",
    "                        return_attention_mask = True,   # Attention maskの作成\n",
    "                        return_tensors = 'pt',     #  Pytorch tensorsで返す\n",
    "                   )\n",
    "\n",
    "    input_ids_val.append(encoded_dict_val['input_ids'])\n",
    "    attention_masks_val.append(encoded_dict_val['attention_mask']) # 実質的なトークンがある部分のみ１になる\n",
    "\n",
    "#input_ids_train.append(sentences_train['input_ids'])\n",
    "#attention_masks_train.append(sentences_train['attention_mask'])\n",
    "\n",
    "# リストに入ったtensorを縦方向（dim=0）へ結合\n",
    "input_ids_val = torch.cat(input_ids_val, dim=0)\n",
    "attention_masks_val = torch.cat(attention_masks_val, dim=0)\n",
    "\n",
    "# tensor型に変換\n",
    "labels_val = torch.tensor(labels_val)\n",
    "\n",
    "# 確認\n",
    "print('Original: ', text_val[0])\n",
    "print('Token IDs:', input_ids_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  レゴランドの向いにホテルが立ってるんですよー私も今日 初めて行きました；；レストランはビュッフェタイプでした 他にもバーとかありましたよ？少々お高いですが わりと楽しめました！\n",
      "Token IDs: tensor([    2,     1,     1,     1, 29823,  1878,  3503, 26733, 28656, 11716,\n",
      "            3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "input_ids_test = []\n",
    "attention_masks_test = []\n",
    "max_len = 128\n",
    "\n",
    "# 1文づつ処理\n",
    "for sentence in text_test:\n",
    "    encoded_dict_test = bert_tokenizer.encode_plus( # input_ids, token_type_ids, attention_mask を出力する\n",
    "                        sentence,                      \n",
    "                        add_special_tokens = True, # Special Tokenの追加\n",
    "                        max_length = max_len + 2,           # 文章の長さを固定（Padding/Trancatinating）\n",
    "                        pad_to_max_length = True,# PADDINGで埋める\n",
    "                        return_attention_mask = True,   # Attention maskの作成\n",
    "                        return_tensors = 'pt',     #  Pytorch tensorsで返す\n",
    "                   )\n",
    "\n",
    "    input_ids_test.append(encoded_dict_test['input_ids'])\n",
    "    attention_masks_test.append(encoded_dict_test['attention_mask']) # 実質的なトークンがある部分のみ１になる\n",
    "\n",
    "#input_ids_train.append(sentences_train['input_ids'])\n",
    "#attention_masks_train.append(sentences_train['attention_mask'])\n",
    "\n",
    "# リストに入ったtensorを縦方向（dim=0）へ結合\n",
    "input_ids_test = torch.cat(input_ids_test, dim=0)\n",
    "attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
    "\n",
    "# tensor型に変換\n",
    "labels_test = torch.tensor(labels_test)\n",
    "\n",
    "# 確認\n",
    "print('Original: ', text_test[0])\n",
    "print('Token IDs:', input_ids_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データローダーの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train) # input_ids_train, attention_masks_train, labels_train を使ってデータセットを作成\n",
    "val_dataset = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "\n",
    "# データローダーの作成\n",
    "batch_size = 16\n",
    "\n",
    "# datsets = [データセット全て]\n",
    "# Dataloader = [[batch_1], [batch_2], ... [batch_n]]\n",
    "\n",
    "# 訓練データローダー\n",
    "train_dataloader = DataLoader( # Dataloader : データセットからデータをバッチサイズに固めて返すモジュール\n",
    "    train_dataset,  \n",
    "    sampler = RandomSampler(train_dataset), # ランダムにデータを取得してバッチ化（sampler : datasetsのバッチの固め方を決める事のできる設定）\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "# 検証データローダー\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    sampler = SequentialSampler(val_dataset), # 順番にデータを取得してバッチ化\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "# テストデータローダー\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    sampler = SequentialSampler(test_dataset), # 順番にデータを取得してバッチ化\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/sakulab/workspace/yaguchi/research/Master/code/LLM/Japanese_L-12_H-768_A-12_E-30_BPE 2/pytorch_model.bin were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_json_file(config_file)\n",
    "model = BertModel.from_pretrained(model_file,config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BertClassifier, self).__init__()\n",
    "    self.bert = model\n",
    "    \n",
    "    # BERTの隠れ層の次元数は768, カテゴリ数が2\n",
    "    self.linear = nn.Linear(768, 2)\n",
    "    # 重み初期化処理\n",
    "    nn.init.normal_(self.linear.weight, std=0.02)\n",
    "    nn.init.normal_(self.linear.bias, 0)\n",
    "\n",
    "  def forward(self, input_ids):\n",
    "    # last_hidden_stateを受け取る\n",
    "    vec = self.bert(input_ids)\n",
    "    # 先頭トークンclsのベクトルだけ取得\n",
    "    vec = vec[0][:,0,:]   #変更\n",
    "    vec = vec.view(-1, 768)\n",
    "    # 全結合層でクラス分類用に次元を変換\n",
    "    out = self.linear(vec)\n",
    "    \n",
    "    return out\n",
    "\n",
    "model = BertClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#ファインチューニング\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in model.bert.encoder.layer[-1].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, param in model.linear.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 最適化手法の設定\n",
    "\"\"\"\n",
    "optimizer = Adam([\n",
    "    {'params': model.bert.encoder.layer[-1].parameters(), 'lr': 2e-5},\n",
    "    {'params': model.linear.parameters(), 'lr': 2e-5}\n",
    "], betas=(0.9, 0.999))\n",
    "\"\"\"\n",
    "#最適化関数の設定\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "EarlyStopping_path = '/home/sakulab/workspace/yaguchi/research/Master/code/LLM/BERT/京大bert/checkpoint/val5(epoch10)_bert_checkpoint_model.pth'\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"earlystoppingクラス\"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, verbose=False, path= EarlyStopping_path ):\n",
    "        \"\"\"引数：最小値の非更新数カウンタ、表示設定、モデル格納path\"\"\"\n",
    "\n",
    "        self.patience = patience    #設定ストップカウンタ\n",
    "        self.verbose = verbose      #表示の有無\n",
    "        self.counter = 0            #現在のカウンタ値\n",
    "        self.best_score = None      #ベストスコア\n",
    "        self.early_stop = False     #ストップフラグ\n",
    "        self.val_loss_min = np.Inf   #前回のベストスコア記憶用\n",
    "        self.path = path             #ベストモデル格納path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        特殊(call)メソッド\n",
    "        実際に学習ループ内で最小lossを更新したか否かを計算させる部分\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:  #1Epoch目の処理\n",
    "            self.best_score = score   #1Epoch目はそのままベストスコアとして記録する\n",
    "            self.checkpoint(val_loss, model)  #記録後にモデルを保存してスコア表示する\n",
    "        elif score < self.best_score:  # ベストスコアを更新できなかった場合\n",
    "            self.counter += 1   #ストップカウンタを+1\n",
    "            if self.verbose:  #表示を有効にした場合は経過を表示\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')  #現在のカウンタを表示する \n",
    "            if self.counter >= self.patience:  #設定カウントを上回ったらストップフラグをTrueに変更\n",
    "                self.early_stop = True\n",
    "        else:  #ベストスコアを更新した場合\n",
    "            self.best_score = score  #ベストスコアを上書き\n",
    "            self.checkpoint(val_loss, model)  #モデルを保存してスコア表示\n",
    "            self.counter = 0  #ストップカウンタリセット\n",
    "\n",
    "    def checkpoint(self, val_loss, model):\n",
    "        '''ベストスコア更新時に実行されるチェックポイント関数'''\n",
    "        if self.verbose:  #表示を有効にした場合は、前回のベストスコアからどれだけ更新したか？を表示\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)  #ベストモデルを指定したpathに保存\n",
    "        self.val_loss_min = val_loss  #その時のlossを記録する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習結果の保存用\n",
    "history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc':[]\n",
    "    }\n",
    "\n",
    "def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    #★EarlyStoppingクラスのインスタンス化とエアearylistoppingの設定\n",
    "    earlystopping = EarlyStopping(patience=15)\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    \n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    model.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # モデルを訓練モードに\n",
    "               \n",
    "            else:\n",
    "                model.eval()   # モデルを検証モードに\n",
    "\n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "\n",
    "            # 開始時刻を保存\n",
    "            t_epoch_start = time.time()\n",
    "            t_iter_start = time.time()\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                #inputs = batch.Text[0].to(device)  # 文章 input_ids\n",
    "                inputs = batch[0].to(device)  # 文章 input_ids\n",
    "                #labels = batch.Label.to(device)  # ラベル\n",
    "                labels = batch[2].to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # Bertに入力\n",
    "                    outputs= model(inputs)\n",
    "                    \n",
    "                    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                       \n",
    "                      \n",
    "                    # 損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "      \n",
    "            # epochごとのlossと正解率\n",
    "            t_epoch_finish = time.time()\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double(\n",
    "            ) / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "            t_epoch_start = time.time()\n",
    "\n",
    "            \n",
    "            if phase == 'train':\n",
    "                 history['train_loss'].append(epoch_loss)\n",
    "                 history['train_acc'].append(epoch_acc)\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)   \n",
    "                history['val_acc'].append(epoch_acc)\n",
    "                \n",
    "        earlystopping(epoch_loss, model) #callメソッド呼び出し\n",
    "        if earlystopping.early_stop: #ストップフラグがTrueの場合、breakでforループを抜ける\n",
    "              print(\"Early Stopping!\")\n",
    "              break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cuda:0\n",
      "Epoch 1/10 | train |  Loss: 0.5377 Acc: 0.7523\n",
      "Epoch 1/10 |  val  |  Loss: 0.5156 Acc: 0.7450\n",
      "Epoch 2/10 | train |  Loss: 0.4892 Acc: 0.7777\n",
      "Epoch 2/10 |  val  |  Loss: 0.5026 Acc: 0.7681\n",
      "Epoch 3/10 | train |  Loss: 0.4661 Acc: 0.7916\n",
      "Epoch 3/10 |  val  |  Loss: 0.4616 Acc: 0.7932\n",
      "Epoch 4/10 | train |  Loss: 0.4458 Acc: 0.8026\n",
      "Epoch 4/10 |  val  |  Loss: 0.4762 Acc: 0.7841\n",
      "Epoch 5/10 | train |  Loss: 0.4357 Acc: 0.8110\n",
      "Epoch 5/10 |  val  |  Loss: 0.4611 Acc: 0.7892\n",
      "Epoch 6/10 | train |  Loss: 0.4187 Acc: 0.8156\n",
      "Epoch 6/10 |  val  |  Loss: 0.4536 Acc: 0.7962\n",
      "Epoch 7/10 | train |  Loss: 0.4069 Acc: 0.8241\n",
      "Epoch 7/10 |  val  |  Loss: 0.4565 Acc: 0.7811\n",
      "Epoch 8/10 | train |  Loss: 0.3968 Acc: 0.8296\n",
      "Epoch 8/10 |  val  |  Loss: 0.4517 Acc: 0.7882\n",
      "Epoch 9/10 | train |  Loss: 0.3809 Acc: 0.8403\n",
      "Epoch 9/10 |  val  |  Loss: 0.4768 Acc: 0.7801\n",
      "Epoch 10/10 | train |  Loss: 0.3681 Acc: 0.8430\n",
      "Epoch 10/10 |  val  |  Loss: 0.4737 Acc: 0.7952\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": validation_dataloader}\n",
    "model_trained = train_model(model, dataloaders_dict,\n",
    "                          criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ベストモデルを適応させる\n",
    "model_trained.load_state_dict(torch.load(EarlyStopping_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正解率の算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:03<00:00, 33.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テストデータ1998個での正解率：0.8108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# テストデータでの正解率を求める\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_trained.eval()   # モデルを検証モードに\n",
    "model_trained.to(device)  # GPUが使えるならGPUへ送る\n",
    "\n",
    "# epochの正解数を記録する変数\n",
    "epoch_corrects = 0\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "for batch in tqdm(test_dataloader):  # testデータのDataLoader\n",
    "    # batchはTextとLableの辞書オブジェクト\n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #inputs = batch.Text[0].to(device)  # 文章\n",
    "    inputs = batch[0].to(device)  # 文章\n",
    "    #labels = batch.Label.to(device)  # ラベル\n",
    "    labels = batch[2].to(device)  # ラベル\n",
    "\n",
    "    # 順伝搬（forward）計算\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "       \n",
    "        outputs = model_trained(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "\n",
    "        if y_pred == []:\n",
    "          y_pred = preds\n",
    "        else:\n",
    "          y_pred = torch.cat([y_pred, preds], dim=0)\n",
    "        \n",
    "        epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n",
    "\n",
    "# 正解率\n",
    "epoch_acc = epoch_corrects.double() / len(test_dataloader.dataset)\n",
    "\n",
    "print('テストデータ{}個での正解率：{:.4f}'.format(len(test_dataloader.dataset), epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類評価指標4つでの結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8108\n",
      "precision = 0.6533\n",
      "recall = 0.4701\n",
      "f1 score = 0.5468\n"
     ]
    }
   ],
   "source": [
    "# 評価指標の算出\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 評価指標算出メソッドの作成\n",
    "def print_metrics_by_cutoff(y, y_pred, model, cutoff):\n",
    "    \"\"\"\n",
    "    指定した閾値に基づくAccuracy, Precision, Recall, F1-scoreを出力する\n",
    "    \"\"\"\n",
    "    #y_pred = model.predict_proba(X)[:, 1] >= cutoff\n",
    "    print('accuracy = {:.4f}'.format(accuracy_score(y_true=y, y_pred=y_pred)))\n",
    "    print('precision = {:.4f}'.format(precision_score(y_true=y, y_pred=y_pred)))\n",
    "    print('recall = {:.4f}'.format(recall_score(y_true=y, y_pred=y_pred)))\n",
    "    print('f1 score = {:.4f}'.format(f1_score(y_true=y, y_pred=y_pred)))\n",
    "    # Accuracy, Precision, Recall, F1-score\n",
    "print_metrics_by_cutoff(y=labels_test, y_pred=y_pred, model=model_trained, cutoff=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPUのメモリ解放\n",
    "! kill -9 $(lsof -t /dev/nvidia*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 30 14:00:38 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:81:00.0  On |                  N/A |\n",
      "| 72%   62C    P0   133W / 350W |    322MiB / 24576MiB |      8%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:C1:00.0 Off |                  N/A |\n",
      "| 42%   50C    P8    20W / 350W |     10MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2157250      G   /usr/lib/xorg/Xorg                 53MiB |\n",
      "|    0   N/A  N/A   2815414      G   /usr/lib/xorg/Xorg                142MiB |\n",
      "|    0   N/A  N/A   2967116      G   /usr/bin/gnome-shell              109MiB |\n",
      "|    1   N/A  N/A   2157250      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   2815414      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[yaguchi]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
