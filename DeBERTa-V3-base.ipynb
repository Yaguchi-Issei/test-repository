{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#GPU使用可能確認\n",
    "print(torch.cuda.is_available())\n",
    "#使用できるデバイス数確認\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sun Jul 30 15:40:03 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:81:00.0  On |                  N/A |\n",
      "| 48%   51C    P8    41W / 350W |   7785MiB / 24576MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:C1:00.0 Off |                  N/A |\n",
      "| 43%   47C    P8    18W / 350W |   5700MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2157250      G   /usr/lib/xorg/Xorg                 53MiB |\n",
      "|    0   N/A  N/A   2815414      G   /usr/lib/xorg/Xorg                150MiB |\n",
      "|    0   N/A  N/A   2967116      G   /usr/bin/gnome-shell               84MiB |\n",
      "|    0   N/A  N/A   2967940      C   ...envs/[yaguchi]/bin/python     7477MiB |\n",
      "|    1   N/A  N/A   2157250      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   2815414      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   2967940      C   ...envs/[yaguchi]/bin/python     5687MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification, AdamW, AutoTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import MeCab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形態素解析器の初期化\n",
    "m = MeCab.Tagger(\"-Owakati\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語のベクトル化（ID化）はAutoTokenizerのencode_plusメソッドを使用して行われている。\n",
    "具体的にはTextClassificationDatasetクラスの__getitem__メソッド内で行われる。\n",
    "以下参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nencoding = self.tokenizer.encode_plus(\\n    text,\\n    add_special_tokens=True,\\n    max_length=self.max_len,\\n    return_token_type_ids=False,\\n    padding='max_length',\\n    return_attention_mask=True,\\n    return_tensors='pt',\\n    truncation=True\\n)\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "encoding = self.tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=self.max_len,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    truncation=True\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットクラスの定義\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        # MeCabを用いて形態素解析\n",
    "        text = m.parse(text)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUの設定\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価指標の計算\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeBERTa V3のトークナイザーは特定のものを指定せず、モデルの名前を直接使用\n",
    "https://huggingface.co/microsoft/deberta-v3-baseを用いてpre-traind \n",
    "また、gitに公開しているコードはこちらhttps://github.com/microsoft/DeBERTa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaConfig, DebertaForSequenceClassification, AutoTokenizer,AutoModelForSequenceClassification\n",
    "# トークナイザーの初期化\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "train_data = pd.read_csv('/home/sakulab/workspace/yaguchi/research/Master/data/意見データ(5分割交差検証用)/train_data(5).csv')  # 訓練データのパス\n",
    "val_data = pd.read_csv('/home/sakulab/workspace/yaguchi/research/Master/data/意見データ(5分割交差検証用)/val_data(5).csv')  # バリデーションデータのパス\n",
    "test_data = pd.read_csv('/home/sakulab/workspace/yaguchi/research/Master/data/意見データ(5分割交差検証用)/test_data(5).csv')  # テストデータのパス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの前処理\n",
    "train_texts = train_data['text'].tolist()\n",
    "train_labels = train_data['label'].tolist()\n",
    "test_texts = test_data['text'].tolist()\n",
    "test_labels = test_data['label'].tolist()\n",
    "val_texts = val_data['text'].tolist()\n",
    "val_labels = val_data['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_len=128)\n",
    "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_len=128)\n",
    "test_dataset = TextClassificationDataset(test_texts, test_labels, tokenizer, max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# モデルの初期化\n",
    "model = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base', num_labels=len(set(train_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '中', '野', 'キ', 'ャ', 'ン', 'パ', 'ス', 'は', '狭', 'い']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('中野キャンパスは狭い')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# トレーニングの設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/checkpoint_val_5(epoch10)',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs/val_5(epoch10)',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 早期停止の設定\n",
    "early_stopping_rounds = 5\n",
    "no_improve_in_last_n_rounds = 0\n",
    "best_val_loss = np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最適化関数：AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最適化関数の設定\n",
    "from transformers import AdamW\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.args.learning_rate\n",
    "        )\n",
    "        self.lr_scheduler = get_scheduler(\n",
    "            \"linear\",\n",
    "            optimizer=self.optimizer,\n",
    "            num_warmup_steps=self.args.warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2190' max='2190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2190/2190 31:28, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.671900</td>\n",
       "      <td>0.668047</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.660900</td>\n",
       "      <td>0.657203</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>0.639275</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.622900</td>\n",
       "      <td>0.609527</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.590600</td>\n",
       "      <td>0.574740</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.571400</td>\n",
       "      <td>0.551309</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.527222</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.463500</td>\n",
       "      <td>0.528014</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.566100</td>\n",
       "      <td>0.527951</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.533500</td>\n",
       "      <td>0.515209</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.520700</td>\n",
       "      <td>0.529186</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.487000</td>\n",
       "      <td>0.534225</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.538200</td>\n",
       "      <td>0.518396</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.510554</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>0.532673</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.531300</td>\n",
       "      <td>0.505102</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.497100</td>\n",
       "      <td>0.499346</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>0.494091</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.503900</td>\n",
       "      <td>0.483070</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.478057</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.407800</td>\n",
       "      <td>0.578479</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.505500</td>\n",
       "      <td>0.463150</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.481200</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.457872</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.264516</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.162698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.470800</td>\n",
       "      <td>0.455981</td>\n",
       "      <td>0.779116</td>\n",
       "      <td>0.547325</td>\n",
       "      <td>0.568376</td>\n",
       "      <td>0.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.522100</td>\n",
       "      <td>0.467518</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.396825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.478053</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.433200</td>\n",
       "      <td>0.461302</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.446500</td>\n",
       "      <td>0.488398</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.458800</td>\n",
       "      <td>0.462843</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.458600</td>\n",
       "      <td>0.513943</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.494500</td>\n",
       "      <td>0.461496</td>\n",
       "      <td>0.773092</td>\n",
       "      <td>0.602113</td>\n",
       "      <td>0.541139</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.449600</td>\n",
       "      <td>0.498950</td>\n",
       "      <td>0.785141</td>\n",
       "      <td>0.454082</td>\n",
       "      <td>0.635714</td>\n",
       "      <td>0.353175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.457800</td>\n",
       "      <td>0.493395</td>\n",
       "      <td>0.756024</td>\n",
       "      <td>0.588832</td>\n",
       "      <td>0.513274</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.420600</td>\n",
       "      <td>0.466458</td>\n",
       "      <td>0.769076</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.182540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.468200</td>\n",
       "      <td>0.476965</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>0.463118</td>\n",
       "      <td>0.778112</td>\n",
       "      <td>0.289389</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>0.178571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.477766</td>\n",
       "      <td>0.796185</td>\n",
       "      <td>0.455764</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.337302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.440388</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.547619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.486800</td>\n",
       "      <td>0.462749</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.589474</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.411600</td>\n",
       "      <td>0.450060</td>\n",
       "      <td>0.792169</td>\n",
       "      <td>0.547046</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.496032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.416700</td>\n",
       "      <td>0.557955</td>\n",
       "      <td>0.748996</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.443200</td>\n",
       "      <td>0.457798</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>0.446462</td>\n",
       "      <td>0.793173</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.643750</td>\n",
       "      <td>0.408730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.450500</td>\n",
       "      <td>0.552017</td>\n",
       "      <td>0.728916</td>\n",
       "      <td>0.582043</td>\n",
       "      <td>0.477157</td>\n",
       "      <td>0.746032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.457259</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.474227</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.365079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.423800</td>\n",
       "      <td>0.460044</td>\n",
       "      <td>0.793173</td>\n",
       "      <td>0.520930</td>\n",
       "      <td>0.629213</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.412400</td>\n",
       "      <td>0.447818</td>\n",
       "      <td>0.788153</td>\n",
       "      <td>0.486618</td>\n",
       "      <td>0.628931</td>\n",
       "      <td>0.396825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.408300</td>\n",
       "      <td>0.487352</td>\n",
       "      <td>0.757028</td>\n",
       "      <td>0.584192</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.674603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.446400</td>\n",
       "      <td>0.457825</td>\n",
       "      <td>0.787149</td>\n",
       "      <td>0.509259</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.436508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>0.562693</td>\n",
       "      <td>0.776104</td>\n",
       "      <td>0.545825</td>\n",
       "      <td>0.560669</td>\n",
       "      <td>0.531746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.540300</td>\n",
       "      <td>0.474919</td>\n",
       "      <td>0.797189</td>\n",
       "      <td>0.512077</td>\n",
       "      <td>0.654321</td>\n",
       "      <td>0.420635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.430800</td>\n",
       "      <td>0.461738</td>\n",
       "      <td>0.787149</td>\n",
       "      <td>0.608856</td>\n",
       "      <td>0.568966</td>\n",
       "      <td>0.654762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.388000</td>\n",
       "      <td>0.441417</td>\n",
       "      <td>0.788153</td>\n",
       "      <td>0.514943</td>\n",
       "      <td>0.612022</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.455172</td>\n",
       "      <td>0.766064</td>\n",
       "      <td>0.204778</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.119048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.418300</td>\n",
       "      <td>0.491828</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.630952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.383300</td>\n",
       "      <td>0.452561</td>\n",
       "      <td>0.791165</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>0.644737</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.459400</td>\n",
       "      <td>0.446818</td>\n",
       "      <td>0.798193</td>\n",
       "      <td>0.535797</td>\n",
       "      <td>0.640884</td>\n",
       "      <td>0.460317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.402200</td>\n",
       "      <td>0.458080</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.429348</td>\n",
       "      <td>0.681034</td>\n",
       "      <td>0.313492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.410900</td>\n",
       "      <td>0.441608</td>\n",
       "      <td>0.788153</td>\n",
       "      <td>0.468514</td>\n",
       "      <td>0.641379</td>\n",
       "      <td>0.369048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.439300</td>\n",
       "      <td>0.504084</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.417800</td>\n",
       "      <td>0.439730</td>\n",
       "      <td>0.792169</td>\n",
       "      <td>0.505967</td>\n",
       "      <td>0.634731</td>\n",
       "      <td>0.420635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.413900</td>\n",
       "      <td>0.453504</td>\n",
       "      <td>0.787149</td>\n",
       "      <td>0.485437</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.396825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.430500</td>\n",
       "      <td>0.450218</td>\n",
       "      <td>0.756024</td>\n",
       "      <td>0.531792</td>\n",
       "      <td>0.516854</td>\n",
       "      <td>0.547619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.431400</td>\n",
       "      <td>0.477030</td>\n",
       "      <td>0.737952</td>\n",
       "      <td>0.581059</td>\n",
       "      <td>0.487871</td>\n",
       "      <td>0.718254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.442800</td>\n",
       "      <td>0.446244</td>\n",
       "      <td>0.800201</td>\n",
       "      <td>0.522782</td>\n",
       "      <td>0.660606</td>\n",
       "      <td>0.432540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.379600</td>\n",
       "      <td>0.448995</td>\n",
       "      <td>0.766064</td>\n",
       "      <td>0.596187</td>\n",
       "      <td>0.529231</td>\n",
       "      <td>0.682540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.325100</td>\n",
       "      <td>0.463798</td>\n",
       "      <td>0.788153</td>\n",
       "      <td>0.578842</td>\n",
       "      <td>0.582329</td>\n",
       "      <td>0.575397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.374900</td>\n",
       "      <td>0.466330</td>\n",
       "      <td>0.800201</td>\n",
       "      <td>0.501253</td>\n",
       "      <td>0.680272</td>\n",
       "      <td>0.396825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.394100</td>\n",
       "      <td>0.486808</td>\n",
       "      <td>0.763052</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.522989</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>0.457394</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.309524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.482792</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.595122</td>\n",
       "      <td>0.504132</td>\n",
       "      <td>0.726190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.398500</td>\n",
       "      <td>0.440707</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.539683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.325500</td>\n",
       "      <td>0.485469</td>\n",
       "      <td>0.802209</td>\n",
       "      <td>0.527578</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.436508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>0.514802</td>\n",
       "      <td>0.743976</td>\n",
       "      <td>0.607088</td>\n",
       "      <td>0.496222</td>\n",
       "      <td>0.781746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.388000</td>\n",
       "      <td>0.427064</td>\n",
       "      <td>0.805221</td>\n",
       "      <td>0.522167</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.420635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.352300</td>\n",
       "      <td>0.522295</td>\n",
       "      <td>0.738956</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.489691</td>\n",
       "      <td>0.753968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>0.543048</td>\n",
       "      <td>0.768072</td>\n",
       "      <td>0.595447</td>\n",
       "      <td>0.532915</td>\n",
       "      <td>0.674603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.345800</td>\n",
       "      <td>0.499119</td>\n",
       "      <td>0.806225</td>\n",
       "      <td>0.575824</td>\n",
       "      <td>0.645320</td>\n",
       "      <td>0.519841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.524759</td>\n",
       "      <td>0.704819</td>\n",
       "      <td>0.576369</td>\n",
       "      <td>0.452489</td>\n",
       "      <td>0.793651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.418100</td>\n",
       "      <td>0.457932</td>\n",
       "      <td>0.799197</td>\n",
       "      <td>0.598394</td>\n",
       "      <td>0.605691</td>\n",
       "      <td>0.591270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.410200</td>\n",
       "      <td>0.502348</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.583871</td>\n",
       "      <td>0.491848</td>\n",
       "      <td>0.718254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>0.465782</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.595506</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.630952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.528975</td>\n",
       "      <td>0.791165</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.463900</td>\n",
       "      <td>0.457652</td>\n",
       "      <td>0.780120</td>\n",
       "      <td>0.581262</td>\n",
       "      <td>0.560886</td>\n",
       "      <td>0.603175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.406700</td>\n",
       "      <td>0.445682</td>\n",
       "      <td>0.791165</td>\n",
       "      <td>0.613383</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.654762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.468899</td>\n",
       "      <td>0.802209</td>\n",
       "      <td>0.532067</td>\n",
       "      <td>0.662722</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.336200</td>\n",
       "      <td>0.430774</td>\n",
       "      <td>0.794177</td>\n",
       "      <td>0.541387</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>0.480159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.395800</td>\n",
       "      <td>0.469962</td>\n",
       "      <td>0.758032</td>\n",
       "      <td>0.586621</td>\n",
       "      <td>0.516616</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.303100</td>\n",
       "      <td>0.620372</td>\n",
       "      <td>0.805221</td>\n",
       "      <td>0.526829</td>\n",
       "      <td>0.683544</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.478947</td>\n",
       "      <td>0.773092</td>\n",
       "      <td>0.581481</td>\n",
       "      <td>0.545139</td>\n",
       "      <td>0.623016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.314100</td>\n",
       "      <td>0.501760</td>\n",
       "      <td>0.761044</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.486824</td>\n",
       "      <td>0.788153</td>\n",
       "      <td>0.585462</td>\n",
       "      <td>0.579767</td>\n",
       "      <td>0.591270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.331900</td>\n",
       "      <td>0.483435</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.332000</td>\n",
       "      <td>0.468405</td>\n",
       "      <td>0.784137</td>\n",
       "      <td>0.547368</td>\n",
       "      <td>0.582960</td>\n",
       "      <td>0.515873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.294300</td>\n",
       "      <td>0.556101</td>\n",
       "      <td>0.776104</td>\n",
       "      <td>0.587800</td>\n",
       "      <td>0.550173</td>\n",
       "      <td>0.630952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.556902</td>\n",
       "      <td>0.758032</td>\n",
       "      <td>0.600332</td>\n",
       "      <td>0.515670</td>\n",
       "      <td>0.718254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.495475</td>\n",
       "      <td>0.766064</td>\n",
       "      <td>0.572477</td>\n",
       "      <td>0.532423</td>\n",
       "      <td>0.619048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>0.507899</td>\n",
       "      <td>0.800201</td>\n",
       "      <td>0.566449</td>\n",
       "      <td>0.628019</td>\n",
       "      <td>0.515873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.302000</td>\n",
       "      <td>0.503560</td>\n",
       "      <td>0.790161</td>\n",
       "      <td>0.550538</td>\n",
       "      <td>0.600939</td>\n",
       "      <td>0.507937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.335800</td>\n",
       "      <td>0.460675</td>\n",
       "      <td>0.778112</td>\n",
       "      <td>0.575816</td>\n",
       "      <td>0.557621</td>\n",
       "      <td>0.595238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.288100</td>\n",
       "      <td>0.503117</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.581882</td>\n",
       "      <td>0.518634</td>\n",
       "      <td>0.662698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.394000</td>\n",
       "      <td>0.482437</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.569811</td>\n",
       "      <td>0.543165</td>\n",
       "      <td>0.599206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.256800</td>\n",
       "      <td>0.496028</td>\n",
       "      <td>0.787149</td>\n",
       "      <td>0.560166</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.318100</td>\n",
       "      <td>0.507002</td>\n",
       "      <td>0.767068</td>\n",
       "      <td>0.576642</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>0.626984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.514254</td>\n",
       "      <td>0.772088</td>\n",
       "      <td>0.557505</td>\n",
       "      <td>0.547893</td>\n",
       "      <td>0.567460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.318800</td>\n",
       "      <td>0.483921</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.552727</td>\n",
       "      <td>0.510067</td>\n",
       "      <td>0.603175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.314600</td>\n",
       "      <td>0.487326</td>\n",
       "      <td>0.784137</td>\n",
       "      <td>0.562118</td>\n",
       "      <td>0.577406</td>\n",
       "      <td>0.547619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.323700</td>\n",
       "      <td>0.509978</td>\n",
       "      <td>0.754016</td>\n",
       "      <td>0.599018</td>\n",
       "      <td>0.509749</td>\n",
       "      <td>0.726190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.289100</td>\n",
       "      <td>0.550421</td>\n",
       "      <td>0.790161</td>\n",
       "      <td>0.493947</td>\n",
       "      <td>0.633540</td>\n",
       "      <td>0.404762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.190100</td>\n",
       "      <td>0.558396</td>\n",
       "      <td>0.778112</td>\n",
       "      <td>0.588454</td>\n",
       "      <td>0.554386</td>\n",
       "      <td>0.626984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.540367</td>\n",
       "      <td>0.767068</td>\n",
       "      <td>0.584229</td>\n",
       "      <td>0.532680</td>\n",
       "      <td>0.646825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.254200</td>\n",
       "      <td>0.521332</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.599315</td>\n",
       "      <td>0.527108</td>\n",
       "      <td>0.694444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.650395</td>\n",
       "      <td>0.792169</td>\n",
       "      <td>0.486352</td>\n",
       "      <td>0.649007</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.305200</td>\n",
       "      <td>0.561241</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.595960</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.702381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.222700</td>\n",
       "      <td>0.554062</td>\n",
       "      <td>0.778112</td>\n",
       "      <td>0.560636</td>\n",
       "      <td>0.561753</td>\n",
       "      <td>0.559524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.202200</td>\n",
       "      <td>0.570740</td>\n",
       "      <td>0.756024</td>\n",
       "      <td>0.588832</td>\n",
       "      <td>0.513274</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>0.639793</td>\n",
       "      <td>0.782129</td>\n",
       "      <td>0.566866</td>\n",
       "      <td>0.570281</td>\n",
       "      <td>0.563492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.658403</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.504464</td>\n",
       "      <td>0.576531</td>\n",
       "      <td>0.448413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.327200</td>\n",
       "      <td>0.562614</td>\n",
       "      <td>0.754016</td>\n",
       "      <td>0.582624</td>\n",
       "      <td>0.510448</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>0.590055</td>\n",
       "      <td>0.779116</td>\n",
       "      <td>0.537815</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.507937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.215400</td>\n",
       "      <td>0.610754</td>\n",
       "      <td>0.782129</td>\n",
       "      <td>0.510158</td>\n",
       "      <td>0.591623</td>\n",
       "      <td>0.448413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.303400</td>\n",
       "      <td>0.541752</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.508621</td>\n",
       "      <td>0.702381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.513435</td>\n",
       "      <td>0.785141</td>\n",
       "      <td>0.548523</td>\n",
       "      <td>0.585586</td>\n",
       "      <td>0.515873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.247100</td>\n",
       "      <td>0.559965</td>\n",
       "      <td>0.767068</td>\n",
       "      <td>0.570370</td>\n",
       "      <td>0.534722</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.247700</td>\n",
       "      <td>0.614046</td>\n",
       "      <td>0.782129</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.472222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.232000</td>\n",
       "      <td>0.581291</td>\n",
       "      <td>0.766064</td>\n",
       "      <td>0.584670</td>\n",
       "      <td>0.530744</td>\n",
       "      <td>0.650794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.610450</td>\n",
       "      <td>0.780120</td>\n",
       "      <td>0.496552</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.310600</td>\n",
       "      <td>0.621576</td>\n",
       "      <td>0.728916</td>\n",
       "      <td>0.576803</td>\n",
       "      <td>0.476684</td>\n",
       "      <td>0.730159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>0.536831</td>\n",
       "      <td>0.792169</td>\n",
       "      <td>0.524138</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.452381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.315300</td>\n",
       "      <td>0.548874</td>\n",
       "      <td>0.770080</td>\n",
       "      <td>0.568738</td>\n",
       "      <td>0.541219</td>\n",
       "      <td>0.599206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.574612</td>\n",
       "      <td>0.773092</td>\n",
       "      <td>0.556863</td>\n",
       "      <td>0.550388</td>\n",
       "      <td>0.563492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.670346</td>\n",
       "      <td>0.778112</td>\n",
       "      <td>0.560636</td>\n",
       "      <td>0.561753</td>\n",
       "      <td>0.559524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.725174</td>\n",
       "      <td>0.773092</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>0.548872</td>\n",
       "      <td>0.579365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.139500</td>\n",
       "      <td>0.753587</td>\n",
       "      <td>0.760040</td>\n",
       "      <td>0.569369</td>\n",
       "      <td>0.521452</td>\n",
       "      <td>0.626984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.193300</td>\n",
       "      <td>0.670716</td>\n",
       "      <td>0.786145</td>\n",
       "      <td>0.539957</td>\n",
       "      <td>0.592417</td>\n",
       "      <td>0.496032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.654796</td>\n",
       "      <td>0.763052</td>\n",
       "      <td>0.577061</td>\n",
       "      <td>0.526144</td>\n",
       "      <td>0.638889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.760805</td>\n",
       "      <td>0.780120</td>\n",
       "      <td>0.544699</td>\n",
       "      <td>0.572052</td>\n",
       "      <td>0.519841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>0.763613</td>\n",
       "      <td>0.763052</td>\n",
       "      <td>0.588850</td>\n",
       "      <td>0.524845</td>\n",
       "      <td>0.670635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.184700</td>\n",
       "      <td>0.655348</td>\n",
       "      <td>0.779116</td>\n",
       "      <td>0.556452</td>\n",
       "      <td>0.565574</td>\n",
       "      <td>0.547619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>0.681695</td>\n",
       "      <td>0.774096</td>\n",
       "      <td>0.564797</td>\n",
       "      <td>0.550943</td>\n",
       "      <td>0.579365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.108900</td>\n",
       "      <td>0.753564</td>\n",
       "      <td>0.751004</td>\n",
       "      <td>0.572414</td>\n",
       "      <td>0.506098</td>\n",
       "      <td>0.658730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>0.788071</td>\n",
       "      <td>0.760040</td>\n",
       "      <td>0.570916</td>\n",
       "      <td>0.521311</td>\n",
       "      <td>0.630952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>0.798517</td>\n",
       "      <td>0.773092</td>\n",
       "      <td>0.517094</td>\n",
       "      <td>0.560185</td>\n",
       "      <td>0.480159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.162300</td>\n",
       "      <td>0.691895</td>\n",
       "      <td>0.752008</td>\n",
       "      <td>0.570435</td>\n",
       "      <td>0.507740</td>\n",
       "      <td>0.650794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>0.686331</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.504587</td>\n",
       "      <td>0.597826</td>\n",
       "      <td>0.436508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.177800</td>\n",
       "      <td>0.715414</td>\n",
       "      <td>0.764056</td>\n",
       "      <td>0.585538</td>\n",
       "      <td>0.526984</td>\n",
       "      <td>0.658730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.731578</td>\n",
       "      <td>0.793173</td>\n",
       "      <td>0.570833</td>\n",
       "      <td>0.600877</td>\n",
       "      <td>0.543651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.633257</td>\n",
       "      <td>0.780120</td>\n",
       "      <td>0.573099</td>\n",
       "      <td>0.563218</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.228700</td>\n",
       "      <td>0.574084</td>\n",
       "      <td>0.790161</td>\n",
       "      <td>0.565489</td>\n",
       "      <td>0.593886</td>\n",
       "      <td>0.539683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.627714</td>\n",
       "      <td>0.774096</td>\n",
       "      <td>0.564797</td>\n",
       "      <td>0.550943</td>\n",
       "      <td>0.579365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>0.717407</td>\n",
       "      <td>0.770080</td>\n",
       "      <td>0.567108</td>\n",
       "      <td>0.541516</td>\n",
       "      <td>0.595238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.136800</td>\n",
       "      <td>0.704661</td>\n",
       "      <td>0.773092</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>0.548872</td>\n",
       "      <td>0.579365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.126800</td>\n",
       "      <td>0.788050</td>\n",
       "      <td>0.782129</td>\n",
       "      <td>0.554415</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>0.816884</td>\n",
       "      <td>0.773092</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>0.548872</td>\n",
       "      <td>0.579365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.130600</td>\n",
       "      <td>0.771451</td>\n",
       "      <td>0.760040</td>\n",
       "      <td>0.582897</td>\n",
       "      <td>0.520249</td>\n",
       "      <td>0.662698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.163500</td>\n",
       "      <td>0.769740</td>\n",
       "      <td>0.775100</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.112400</td>\n",
       "      <td>0.795867</td>\n",
       "      <td>0.774096</td>\n",
       "      <td>0.582560</td>\n",
       "      <td>0.547038</td>\n",
       "      <td>0.623016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.848847</td>\n",
       "      <td>0.779116</td>\n",
       "      <td>0.580153</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.603175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.120800</td>\n",
       "      <td>0.900426</td>\n",
       "      <td>0.779116</td>\n",
       "      <td>0.580153</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.603175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.960126</td>\n",
       "      <td>0.793173</td>\n",
       "      <td>0.552174</td>\n",
       "      <td>0.610577</td>\n",
       "      <td>0.503968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.896921</td>\n",
       "      <td>0.761044</td>\n",
       "      <td>0.568841</td>\n",
       "      <td>0.523333</td>\n",
       "      <td>0.623016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.090400</td>\n",
       "      <td>0.895509</td>\n",
       "      <td>0.776104</td>\n",
       "      <td>0.558416</td>\n",
       "      <td>0.557312</td>\n",
       "      <td>0.559524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.881025</td>\n",
       "      <td>0.779116</td>\n",
       "      <td>0.566929</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>0.889695</td>\n",
       "      <td>0.767068</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.575397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.763052</td>\n",
       "      <td>0.570909</td>\n",
       "      <td>0.526846</td>\n",
       "      <td>0.623016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.879609</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.576208</td>\n",
       "      <td>0.541958</td>\n",
       "      <td>0.615079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>0.967601</td>\n",
       "      <td>0.775100</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.587302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.159700</td>\n",
       "      <td>1.057625</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.965703</td>\n",
       "      <td>0.764056</td>\n",
       "      <td>0.554080</td>\n",
       "      <td>0.530909</td>\n",
       "      <td>0.579365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>0.829609</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.809455</td>\n",
       "      <td>0.760040</td>\n",
       "      <td>0.556586</td>\n",
       "      <td>0.522648</td>\n",
       "      <td>0.595238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.125500</td>\n",
       "      <td>0.783689</td>\n",
       "      <td>0.766064</td>\n",
       "      <td>0.549323</td>\n",
       "      <td>0.535849</td>\n",
       "      <td>0.563492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.826090</td>\n",
       "      <td>0.764056</td>\n",
       "      <td>0.557439</td>\n",
       "      <td>0.530466</td>\n",
       "      <td>0.587302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.867944</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.554687</td>\n",
       "      <td>0.546154</td>\n",
       "      <td>0.563492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.955846</td>\n",
       "      <td>0.775100</td>\n",
       "      <td>0.546559</td>\n",
       "      <td>0.557851</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>1.069817</td>\n",
       "      <td>0.775100</td>\n",
       "      <td>0.539095</td>\n",
       "      <td>0.559829</td>\n",
       "      <td>0.519841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>1.018732</td>\n",
       "      <td>0.775100</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.987506</td>\n",
       "      <td>0.778112</td>\n",
       "      <td>0.565815</td>\n",
       "      <td>0.560311</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.123700</td>\n",
       "      <td>0.936210</td>\n",
       "      <td>0.769076</td>\n",
       "      <td>0.564394</td>\n",
       "      <td>0.539855</td>\n",
       "      <td>0.591270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.971406</td>\n",
       "      <td>0.779116</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>0.515873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.943962</td>\n",
       "      <td>0.770080</td>\n",
       "      <td>0.562141</td>\n",
       "      <td>0.542435</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.994868</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.546218</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.515873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>0.965531</td>\n",
       "      <td>0.769076</td>\n",
       "      <td>0.575646</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>0.619048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>1.004678</td>\n",
       "      <td>0.775100</td>\n",
       "      <td>0.564202</td>\n",
       "      <td>0.553435</td>\n",
       "      <td>0.575397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>1.047896</td>\n",
       "      <td>0.782129</td>\n",
       "      <td>0.565130</td>\n",
       "      <td>0.570850</td>\n",
       "      <td>0.559524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.119700</td>\n",
       "      <td>1.045056</td>\n",
       "      <td>0.768072</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>1.042594</td>\n",
       "      <td>0.780120</td>\n",
       "      <td>0.552147</td>\n",
       "      <td>0.569620</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>1.041690</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.552419</td>\n",
       "      <td>0.561475</td>\n",
       "      <td>0.543651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>1.078085</td>\n",
       "      <td>0.757028</td>\n",
       "      <td>0.567857</td>\n",
       "      <td>0.516234</td>\n",
       "      <td>0.630952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>1.048999</td>\n",
       "      <td>0.767068</td>\n",
       "      <td>0.545098</td>\n",
       "      <td>0.538760</td>\n",
       "      <td>0.551587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>1.049223</td>\n",
       "      <td>0.776104</td>\n",
       "      <td>0.556660</td>\n",
       "      <td>0.557769</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.083900</td>\n",
       "      <td>1.051488</td>\n",
       "      <td>0.773092</td>\n",
       "      <td>0.553360</td>\n",
       "      <td>0.551181</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>1.018106</td>\n",
       "      <td>0.770080</td>\n",
       "      <td>0.553606</td>\n",
       "      <td>0.544061</td>\n",
       "      <td>0.563492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>1.028877</td>\n",
       "      <td>0.779116</td>\n",
       "      <td>0.563492</td>\n",
       "      <td>0.563492</td>\n",
       "      <td>0.563492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>0.990956</td>\n",
       "      <td>0.779116</td>\n",
       "      <td>0.563492</td>\n",
       "      <td>0.563492</td>\n",
       "      <td>0.563492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>0.132700</td>\n",
       "      <td>0.967618</td>\n",
       "      <td>0.776104</td>\n",
       "      <td>0.560158</td>\n",
       "      <td>0.556863</td>\n",
       "      <td>0.563492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.938373</td>\n",
       "      <td>0.773092</td>\n",
       "      <td>0.562016</td>\n",
       "      <td>0.549242</td>\n",
       "      <td>0.575397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.957894</td>\n",
       "      <td>0.778112</td>\n",
       "      <td>0.562376</td>\n",
       "      <td>0.561265</td>\n",
       "      <td>0.563492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.999279</td>\n",
       "      <td>0.780120</td>\n",
       "      <td>0.557576</td>\n",
       "      <td>0.567901</td>\n",
       "      <td>0.547619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>1.051423</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.539683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>1.046612</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.566406</td>\n",
       "      <td>0.557692</td>\n",
       "      <td>0.575397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>1.045177</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.562753</td>\n",
       "      <td>0.574380</td>\n",
       "      <td>0.551587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>1.046988</td>\n",
       "      <td>0.784137</td>\n",
       "      <td>0.549266</td>\n",
       "      <td>0.582222</td>\n",
       "      <td>0.519841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>1.026885</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.562992</td>\n",
       "      <td>0.558594</td>\n",
       "      <td>0.567460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>1.033552</td>\n",
       "      <td>0.770080</td>\n",
       "      <td>0.557060</td>\n",
       "      <td>0.543396</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>1.054001</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.559184</td>\n",
       "      <td>0.575630</td>\n",
       "      <td>0.543651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>1.049057</td>\n",
       "      <td>0.780120</td>\n",
       "      <td>0.557576</td>\n",
       "      <td>0.567901</td>\n",
       "      <td>0.547619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>1.046932</td>\n",
       "      <td>0.778112</td>\n",
       "      <td>0.558882</td>\n",
       "      <td>0.562249</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>1.057291</td>\n",
       "      <td>0.774096</td>\n",
       "      <td>0.554455</td>\n",
       "      <td>0.553360</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>1.068913</td>\n",
       "      <td>0.774096</td>\n",
       "      <td>0.563107</td>\n",
       "      <td>0.551331</td>\n",
       "      <td>0.575397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>1.081783</td>\n",
       "      <td>0.773092</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>0.548872</td>\n",
       "      <td>0.579365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>1.076159</td>\n",
       "      <td>0.774096</td>\n",
       "      <td>0.563107</td>\n",
       "      <td>0.551331</td>\n",
       "      <td>0.575397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>1.072901</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.557769</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>1.076830</td>\n",
       "      <td>0.776104</td>\n",
       "      <td>0.556660</td>\n",
       "      <td>0.557769</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>1.079035</td>\n",
       "      <td>0.775100</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>1.077673</td>\n",
       "      <td>0.775100</td>\n",
       "      <td>0.557312</td>\n",
       "      <td>0.555118</td>\n",
       "      <td>0.559524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>1.074012</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.554687</td>\n",
       "      <td>0.546154</td>\n",
       "      <td>0.563492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>1.073929</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.554687</td>\n",
       "      <td>0.546154</td>\n",
       "      <td>0.563492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5026600956916809,\n",
       " 'eval_accuracy': 0.7937937937937938,\n",
       " 'eval_f1': 0.5242494226327944,\n",
       " 'eval_precision': 0.5958005249343832,\n",
       " 'eval_recall': 0.46804123711340206,\n",
       " 'eval_runtime': 6.3905,\n",
       " 'eval_samples_per_second': 312.651,\n",
       " 'eval_steps_per_second': 2.504,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# トレーナーの初期化\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# モデルの訓練\n",
    "trainer.train()\n",
    "\n",
    "# モデルの評価\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakulab/anaconda3/envs/[yaguchi]/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss: 0.9117526412010193\n",
      "eval_accuracy: 0.7922922922922923\n",
      "eval_f1: 0.5532831001076427\n",
      "eval_precision: 0.5788288288288288\n",
      "eval_recall: 0.5298969072164949\n",
      "eval_runtime: 6.34\n",
      "eval_samples_per_second: 315.142\n",
      "eval_steps_per_second: 2.524\n"
     ]
    }
   ],
   "source": [
    "# ベストモデルのロード\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained('/home/sakulab/workspace/yaguchi/research/Master/code/LLM/Deberta/results/checkpoint_val_5(epoch10)/checkpoint-2000')\n",
    "\n",
    "# ベストモデルを使用したトレーナーの初期化\n",
    "best_trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ベストモデルの評価\n",
    "best_eval_result = best_trainer.evaluate(test_dataset)\n",
    "\n",
    "# 評価結果の表示\n",
    "for key, value in best_eval_result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 30 16:58:54 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:81:00.0  On |                  N/A |\n",
      "| 46%   52C    P8    40W / 350W |    349MiB / 24576MiB |      7%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:C1:00.0 Off |                  N/A |\n",
      "| 40%   48C    P8    18W / 350W |     10MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2157250      G   /usr/lib/xorg/Xorg                 53MiB |\n",
      "|    0   N/A  N/A   2815414      G   /usr/lib/xorg/Xorg                150MiB |\n",
      "|    0   N/A  N/A   2984343      G   /usr/bin/gnome-shell              128MiB |\n",
      "|    1   N/A  N/A   2157250      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   2815414      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPUのメモリ解放\n",
    "! kill -9 $(lsof -t /dev/nvidia*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[yaguchi]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
